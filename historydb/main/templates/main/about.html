<!-- Author: Younghyun Cho <younghyun@berkeley.edu> -->

<!doctype html>

{% extends 'main/base.html' %}
{% load static %}

{% block content %}

<script>
  $(document).ready(function () {
    $('#nav-item-about').addClass('active');
  });
</script>

<div class="container">
  <br><br>
  <h1>GPTune</h1>
  <br>
  <p class="lead">
  <a href="https://github.com/gptune/GPTune">GPTune</a> is an autotuning framework that relies on multitask and transfer learning to help solve the underlying black-box optimization problem using Bayesian optimization methodologies.
  In particular, GPTune is designed to tune high-performance application codes as "black-boxes", running them for carefully chosen tuning parameter values and building a performance model based on the measured performance.
  GPTune provides several unique and advanced autotuning features outlined as follows.
  <ul style="font-size:20px">
    <li><b>High-performance and parallel tuning:</b> GPTune is designed to tune applications running on large-scale cluster systems and can exploit distributed memory parallelism for accelerating surrogate modeling.
      <ul>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/Scalapack-PDGEQRF/scalapack_MLA.py">Example: Autotuning ScaLAPACK's PDGEQRF using distributed parallel surrogate modeling</a></li>
      </ul>
    </li>
    <li>
      <b>Multitask learning-based autotuning:</b> GPTune supports multitask learning-based autotuning that allows us to tune multiple tuning problems simultaneously. Multitask learning would predict an optimal tuning parameter configuration using a fewer number of evaluations compared to single task autotuning by modeling the linear dependence of the multiple tasks.
      <ul>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/Scalapack-PDGEQRF/scalapack_MLA.py">Example: Multitask learning-based autotuning of ScaLAPACK's PDGEQRF</a></li>
      </ul>
    </li>
    <li>
      <b>Transfer learning-based autotuning:</b> GPTune supports transfer learning-based autotuning to tune the given tuning task by leveraging already available performance data collected for different tasks. Different tasks can mean different input problem sizes or the same input problem on different machine and/or software settings.
      <ul>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/Scalapack-PDGEQRF/scalapack_TLA_task.py">Example: Transfer learning from a different input problem for autotuning ScaLAPACK's PDGEQRF</a></li>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/Scalapack-PDGEQRF/scalapack_TLA_machine.py">Example: Transfer learning from a different machine setting for autotuning ScaLAPACK's PDGEQRF</a></li>
      </ul>
    </li>
    <li>
      <b>GPTuneBand (multi-fidelity autotuning):</b> Multi-fidelity tuning uses multiple fidelity levels to guide sampling (generating many cheap samples from lower-fidelity levels). GPTuneBand combines multitask learning with a multi-armed bandit strategy to guide sampling of the given tuning problem.
      <ul>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/STRUMPACK/strumpack_MLA_KRR_MB.py">Example: Multi-fidelity tuning of STRUMPACK's Kernel Ridge Regression (need to set the tuner option to "GPTuneBand")</a></li>
      </ul>
    </li>
    <li>
      <b>Multi-objective tuning:</b> Beyond the classical single-objective tuning, GPTune supports multi-objective tuning that uses NSGA2 algorithm to maximize multiple EI functions for multiple objectives. For an objective, users can also specify whether they want to optimize (minimize) the objective within the given range, or they just want the objective is within the given range.
      <ul>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/SuperLU_DIST/superlu_MLA_MO.py">Example: Multi-objective tuning of SuperLU_DIST</a></li>
      </ul>
    </li>
    <li>
      <b>Unified interface for different autotuners:</b> GPTune uses a unified Python interface and supports using several different autotuners.
      <ul>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/SuperLU_DIST/superlu_MLA_MO.py">Example: Comparing GPTune, HpBandSter, and OpenTuner for a synthetic function</a></li>
      </ul>
    </li>
    <li>
      <b>History database:</b> GPTune supports using an autotuning database called <i>GPTune history database</i> which allows users to save and re-use performance data to reduce the cost of the expensive black-box objective function.
      The history database enables several useful autotuning capabilities, as outlined in the history datbase introduction in below.
    </li>
  </ul>
  </p>

  <br><br>
  <h1>History Database</h1>
  <br>
  <p class="lead">
  The success of autotuning usually depends on collecting sufficient performance data samples to build an accurate surrogate performance model or to explore the search space.
  To enhance reusabiltiy of performance data for autotuning, we provide a history database which is a shared repository and API to share autotuning performance data between multiple users at different sites.
  Our shared repository in this website provides two useful interfaces (1) <a href="{% url 'repo:dashboard' %}">an interactive web dashboard</a> that allows users to navigate the database from a web browser (2) <a href="https://gptune.lbl.gov/docs/src/historydb_repository.html#programmable-api">a programmable API (called crowd-tuning API)</a> which allows users to write a program query to download or upload performance data.
  This website provides <a href="https://gptune.lbl.gov/docs/index.html">an online user guide</a> to learn how to use the database.
  Our history database enables following use cases.
  <ul style="font-size:20px">
    <li>
      <b>Checkpointing and restarting:</b> Users can perform checkpointing and restarting of (long-running) autotuning.
    </li>
    <li>
      <b>Query tuning results:</b> Users can query the best tuning parameter configuration to run their applications.
      <ul>
        <li>Example (web-interface):
          To browse performance data of ScaLAPACK's PDGEQRF in our history database, 1. Access <a href="https://gptune.lbl.gov/repo/dashboard">https://gptune.lbl.gov/repo/dashboard</a>, 2. Choose "PDGEQRF" and click on "Search", 3. You should be able to see the data table that contains the historical data of ScaLAPACK's PDGEQRF.
        </li>
      </ul>
    </li>
    <li>
      <b>Sensitivity analysis:</b> Using historical data, users can run a surrogate model-based sensitivity analysis that estimates importance of each tuning parameter.
      <ul>
        <li>Example (web-interface):
          To run a sensitivity analysis of SuperLU_DIST, 1. Login at http://gptune.lbl.gov, 2. Access <a href="https://gptune.lbl.gov/repo/dashboard">https://gptune.lbl.gov/repo/dashboard</a>, 3. Choose "SuperLU_DIST-pddrive_spawn" and click on "Search", 4. You should be able to see the table, and there should be a button "Sensitivity analysis", you can click on this and follow the instructions there.
        </li>
        <li>Example (crowd-tuning API): <a href="https://github.com/gptune/GPTune/blob/master/examples/CrowdTuning/SuperLU_DIST/sensitivity_analysis.py">Run a sensitivity analysis of SuperLU using crowd-tuning API</a>
        </li>
      </ul>
    </li>
    <li>
      <b>Performance prediction:</b> Using historical data, users can run a surrogate model-based performance prediction for a given tuning parameter configuration.
      <ul>
        <li>Example (web-interface):
          To run a sensitivity analysis of SuperLU_DIST, 1. Login at http://gptune.lbl.gov, 2. access <a href="https://gptune.lbl.gov/repo/dashboard">https://gptune.lbl.gov/repo/dashboard</a>, 3. Choose "SuperLU_DIST-pddrive_spawn" and click on "Search", 4. You should be able to see the table, and there should be a button "Make prediction", you can click on this and follow the instructions there.
        </li>
      </ul>
    </li>
    <li>
      <b>Transfer learning:</b> Users can download historical performance data and use the knowledge to tune a similar but different tuning problem using transfer learning-based autotuning.
      <ul>
        <li>Example: <a href="https://github.com/gptune/GPTune/blob/master/examples/CrowdTuning/Demo/demo_tuning.py">GPTune's transfer learning-based autotuning for a synthetic function using the history database.</a> It uses the crowd-tuning API to access and download historical data from the history database.
        </li>
      </ul>
    </li>
  </ul>

  </p>
  <p class="lead">

  The history database is a part of the GPTune project.
  Performance data in the history database is compatible with GPTune, and the database interfaces are incorporated into GPTune.
  The database feature can also be used by other autotuners (this is a future work).

  <br><br>
  </p>

  <h1>Join our Community</h1>
  <br>
  <p class="lead">
  Unregistered users can only view publicly available performance data.
  This means that they cannot view data disclosed to authorized users or upload any kind of data into out database.
  </p>
  <p class="lead">
  Please <a href="{% url 'account:signup' %}">Sign Up</a> to access more data in our history database!
  Registered users can download data disclosed to registered users.
  There is another privilege level called "certified" for registered users.
  The GPTune team will update the user status as "certified" if the user has entered user profile information correctly.
  Certified users can access data for certified users and create collaboration groups to share data within your group members.
  For more information about our membership, click <a href="{% url 'main:membership' %}">here</a>.
  <br><br>
  </p>

  <h1>About Us</h1>
  <br>
  <p class="lead">
  GPTune is a joint research project between <a href="https://lbl.gov">Lawrence Berkeley National Laboratory</a> and <a href="https://berkeley.edu">University of California at Berkeley</a>, and is part of the <a href="https://xsdk.info/ecp/">xSDK4ECP</a> effort supported by the Exascale Computing Project (ECP).
  <div class="container">
    <a href="https://www.berkeley.edu/"><img src="{% static 'images/UCBerkeleyLogo.png' %}" width=200></a>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://www.lbl.gov/"><img src="{% static 'images/BerkeleyLabLogo.png' %}" width=200></a>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://www.nersc.gov/"><img src="{% static 'images/NERSCLogo.jpg' %}" width=200></a>
  </div>
  </p>

  <br><br>
  <h1>Acknowledgements</h1>
  <br>
  <p class="lead">GPTune Copyright (c) 2019, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from the U.S. Dept. of Energy) and the University of California, Berkeley. All rights reserved.
  <br>
  If you have questions about your rights to use or distribute this software, please contact Berkeley Lab's Intellectual Property Office at IPO@lbl.gov.
  <br>
  NOTICE. This Software was developed under funding from the U.S. Department of Energy and the U.S. Government consequently retains certain rights. As such, the U.S. Government has been granted for itself and others acting on its behalf a paid-up, nonexclusive, irrevocable, worldwide license in the Software to reproduce, distribute copies to the public, prepare derivative works, and perform publicly and display publicly, and to permit other to do so.
  </p>
  <br><br>
</div>

{% endblock %}
