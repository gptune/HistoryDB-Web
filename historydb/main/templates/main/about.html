<!-- Author: Younghyun Cho <younghyun@berkeley.edu> -->

<!doctype html>

{% extends 'main/base.html' %}
{% load static %}

{% block content %}

<script>
  $(document).ready(function () {
    $('#nav-item-about').addClass('active');
  });
</script>

<div class="container">
  <br><br>
  <h1>GPTune</h1>
  <br>
  <p class="lead">
  <a href="https://github.com/gptune/GPTune">GPTune</a> is an autotuning framework that relies on multitask and transfer learning to help solve the underlying black-box optimization problem using Bayesian optimization methodologies.
  In particular, GPTune is designed to tune high-performance application codes as "black-boxes", running them for carefully chosen tuning parameter values and building a performance model based on the measured performance.
  GPTune provides several unique and advanced autotuning features outlined as follows.
  <ul style="font-size:20px">
    <li><b>High-performance and parallel tuning:</b> GPTune is designed to tune applications running on large-scale cluster systems and can exploit distributed memory parallelism for accelerating surrogate modeling.
      <ul>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/Scalapack-PDGEQRF/scalapack_MLA.py">Example: Autotuning ScaLAPACK's PDGEQRF using distributed parallel surrogate modeling</a></li>
      </ul>
    </li>
    <li>
      <b>Multitask learning-based autotuning:</b> GPTune supports multitask learning-based autotuning that allows us to tune multiple tuning problems simultaneously. Multitask learning would predict an optimal tuning parameter configuration using a fewer number of evaluations compared to single task autotuning by modeling the linear dependence of the multiple tasks.
      <ul>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/Scalapack-PDGEQRF/scalapack_MLA.py">Example: Multitask learning-based autotuning of ScaLAPACK's PDGEQRF</a></li>
      </ul>
    </li>
    <li>
      <b>Transfer learning-based autotuning:</b> GPTune supports transfer learning-based autotuning to tune the given tuning task by leveraging already available performance data collected for different tasks. Different tasks can mean different input problem sizes or the same input problem on different machine and/or software settings.
      <ul>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/Scalapack-PDGEQRF/scalapack_TLA_task.py">Example: Transfer learning from a different input problem for autotuning ScaLAPACK's PDGEQRF</a></li>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/Scalapack-PDGEQRF/scalapack_TLA_machine.py">Example: Transfer learning from a different machine setting for autotuning ScaLAPACK's PDGEQRF</a></li>
      </ul>
    </li>
    <li>
      <b>GPTuneBand (multi-fidelity autotuning):</b> Multi-fidelity tuning uses multiple fidelity levels to guide sampling (generating many cheap samples from lower-fidelity levels). GPTuneBand combines multitask learning with a multi-armed bandit strategy to guide sampling of the given tuning problem.
      <ul>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/STRUMPACK/strumpack_MLA_KRR_MB.py">Example: Multi-fidelity tuning of STRUMPACK's Kernel Ridge Regression (need to set the tuner option to "GPTuneBand")</a></li>
      </ul>
    </li>
    <li>
      <b>Multi-objective tuning:</b> Beyond the classical single-objective tuning, GPTune supports multi-objective tuning that uses NSGA2 algorithm to maximize multiple EI functions for multiple objectives. For an objective, users can also specify whether they want to optimize (minimize) the objective within the given range, or they just want the objective is within the given range.
      <ul>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/SuperLU_DIST/superlu_MLA_MO.py">Example: Multi-objective tuning of SuperLU_DIST</a></li>
      </ul>
    </li>
    <li>
      <b>Unified interface for different autotuners:</b> GPTune uses a unified Python interface and supports using several different autotuners.
      <ul>
        <li><a href="https://github.com/gptune/GPTune/blob/master/examples/GPTune-Demo/demo_comparetuners.py">Example: Comparing GPTune, HpBandSter, and OpenTuner for a synthetic function</a></li>
      </ul>
    </li>
    <li>
      <b>History database:</b> GPTune supports using an autotuning database called <i>GPTune history database</i> which allows users to save and re-use performance data to reduce the cost of the expensive black-box objective function.
      The history database enables several useful autotuning capabilities such as transfer learning and performance analysis using pre-trained surrogate models.
    </li>
  </ul>
  </p>

  </p>
  <p class="lead">

  The history database is a part of the GPTune project.
  Performance data in the history database is compatible with GPTune, and the database interfaces are incorporated into GPTune.
  The database feature can also be used by other autotuners (this is a future work).

  <br><br>
  </p>

  <h1>Join our Community</h1>
  <br>
  <p class="lead">
  Unregistered users can only view publicly available performance data.
  This means that they cannot view data disclosed to authorized users or upload any kind of data into out database.
  </p>
  <p class="lead">
  Please <a href="{% url 'account:signup' %}">Sign Up</a> to access more data in our history database!
  Registered users can download data disclosed to registered users.
  There is another privilege level called "certified" for registered users.
  The GPTune team will update the user status as "certified" if the user has entered user profile information correctly.
  Certified users can access data for certified users and create collaboration groups to share data within your group members.
  For more information about our membership, click <a href="{% url 'main:membership' %}">here</a>.
  <br><br>
  </p>

  <h1>About Us</h1>
  <br>
  <p class="lead">
  GPTune is a joint research project between <a href="https://lbl.gov">Lawrence Berkeley National Laboratory</a> and <a href="https://berkeley.edu">University of California at Berkeley</a>, and is part of the <a href="https://xsdk.info/ecp/">xSDK4ECP</a> effort supported by the Exascale Computing Project (ECP).
  <div class="container">
    <a href="https://www.berkeley.edu/"><img src="{% static 'images/UCBerkeleyLogo.png' %}" width=200></a>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://www.lbl.gov/"><img src="{% static 'images/BerkeleyLabLogo.png' %}" width=200></a>
    &nbsp;&nbsp;&nbsp;&nbsp;
    <a href="https://www.nersc.gov/"><img src="{% static 'images/NERSCLogo.jpg' %}" width=200></a>
  </div>
  </p>

  <br><br>
  <h1>Acknowledgements</h1>
  <br>
  <p class="lead">GPTune Copyright (c) 2019, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from the U.S. Dept. of Energy) and the University of California, Berkeley. All rights reserved.
  <br>
  If you have questions about your rights to use or distribute this software, please contact Berkeley Lab's Intellectual Property Office at IPO@lbl.gov.
  <br>
  NOTICE. This Software was developed under funding from the U.S. Department of Energy and the U.S. Government consequently retains certain rights. As such, the U.S. Government has been granted for itself and others acting on its behalf a paid-up, nonexclusive, irrevocable, worldwide license in the Software to reproduce, distribute copies to the public, prepare derivative works, and perform publicly and display publicly, and to permit other to do so.
  </p>
  <br><br>
</div>

{% endblock %}
